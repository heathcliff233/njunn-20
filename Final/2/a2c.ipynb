{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gym[box2d] -q\n!mkdir models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport sys\nfrom itertools import count\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.distributions import Categorical\n\nimport matplotlib.pyplot as plt\nplt.switch_backend('agg')\n\n#import ipdb\n\n# if gpu is to be used\nuse_cuda = torch.cuda.is_available()\n#use_cuda = False\nprint(\"use_cuda : \", use_cuda)\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\nByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\nTensor = FloatTensor\n\nimport gym\nenv = gym.make('LunarLander-v2')\nenv.reset()\n\nclass Actor(nn.Module):\n    def __init__(self, state_size, num_actions):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_size, 16)\n        self.fc2 = nn.Linear(16, 16)\n        self.fc3 = nn.Linear(16, 16)\n        self.fc4 = nn.Linear(16, num_actions)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.log_softmax(self.fc4(x),dim=-1)\n        return x\n\nclass Critic(nn.Module):\n    def __init__(self, state_size):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_size, 16)\n        self.dp1 = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(16, 8)\n        #self.fc3 = nn.Linear(16, 16)\n        self.fc4 = nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dp1(x)\n        x = F.relu(self.fc2(x))\n        #x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nclass A2CAgent(object):\n    def __init__(self, env):\n        super(A2CAgent, self).__init__()\n        self.env = env\n        self.actor = Actor(env.observation_space.shape[0], env.action_space.n)\n        self.critic = Critic(env.observation_space.shape[0])\n        self.actor.cuda()\n        self.critic.cuda()\n        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=5e-4)\n        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=5e-4)\n        self.N_steps = 100\n        self.num_episodes = 40000\n        self.test_episodes = 100\n        #self.num_steps = args.num_steps\n        self.gamma = 0.99\n        self.save_path = \"models\"\n        self.test_freq = 500\n        self.save_freq = 2e3\n        self.train_rewards = []\n        self.test_rewards = []\n        self.train_steps = []\n        self.test_steps = []\n        self.losses_actor = []\n        self.losses_critic = []\n\n    def select_action(self, state):\n        state = Variable(Tensor(state))\n        log_probs = self.actor(state)\n        value = self.critic(state)\n        action = Categorical(log_probs.exp()).sample()\n        #return action.data.cpu().numpy()[0], log_probs[action], value\n        return int(action.data.cpu().numpy()), log_probs[action], value\n\n    def play_episode(self, e):\n        state = self.env.reset()\n        steps = 0\n        rewards = []\n        log_probs = []\n        values = []\n        # for i in range(self.num_steps):\n        while True:\n            action, log_prob, value = self.select_action(state)\n            state, reward, is_terminal, _ = self.env.step(action)\n            log_probs.append(log_prob)\n            rewards.append(reward)\n            values.append(value)\n            steps +=1\n            if is_terminal:\n                break\n        #print(torch.stack(log_probs))\n        #print(log_probs)\n        #print(values)\n        return steps, rewards, torch.stack(log_probs), torch.stack(values)\n\n    def optimize(self, rewards, log_probs, values):\n        T = len(rewards)\n        N = self.N_steps\n        R = np.zeros(T, dtype=np.float32)\n        loss_actor = 0\n        loss_critic = 0\n        for t in reversed(range(T)):\n            V_end = 0 if (t+N >= T) else values[t+N].data\n            R[t] = (self.gamma**N * V_end) + sum([self.gamma**k * rewards[t+k]*1e-2 for k in range(min(N, T-t))])\n        R = Variable(Tensor(R), requires_grad=False)\n        # compute losses using the advantage function;\n        # Note: `values` is detached while computing loss for actor\n        loss_actor = ((R - values.detach()) * -log_probs).mean()\n        loss_critic = ((R - values)**2).mean()\n        # loss = loss_actor + loss_critic\n\n        self.optimizer_actor.zero_grad()\n        self.optimizer_critic.zero_grad()\n        loss_actor.backward()\n        loss_critic.backward()\n        # nn.utils.clip_grad_norm(self.actor.parameters(), grad_norm_limit)\n        # nn.utils.clip_grad_norm(self.critic.parameters(), grad_norm_limit)\n        self.optimizer_actor.step()\n        self.optimizer_critic.step()\n        # self.losses.append(loss.detach().cpu().numpy())\n        # ipdb.set_trace()\n        self.losses_actor.append(int(loss_actor.data.cpu().numpy()))\n        self.losses_critic.append(int(loss_critic.data.cpu().numpy()))\n\n    def train(self, num_episodes):\n        print(\"Going to be training for a total of {} episodes\".format(num_episodes))\n        state = Variable(torch.Tensor(self.env.reset()))\n        for e in range(num_episodes):\n            steps, rewards, log_probs, values = self.play_episode(e)\n            self.train_rewards.append(sum(rewards))\n            self.train_steps.append(steps)\n            self.optimize(rewards, log_probs,values)\n\n            if (e+1) % 100 == 0:\n                print(\"Episode: {}, reward: {}, steps: {}\".format(e+1, sum(rewards), steps))\n\n            # Freeze the current policy and test over 100 episodes\n            if (e+1) % self.test_freq == 0:\n                print(\"-\"*10 + \" testing now \" + \"-\"*10)\n                self.test(self.test_episodes, e)\n\n            # Save the current policy model\n            if (e+1) % (self.save_freq) == 0:\n                torch.save(self.actor.state_dict(),  os.path.join(self.save_path, \"train_actor_ep_{}.pth\".format(e+1)))\n                torch.save(self.critic.state_dict(), os.path.join(self.save_path, \"train_critic_ep_{}.pth\".format(e+1)))\n\n    def test(self, num_episodes, e_train):\n        state = Variable(torch.Tensor(self.env.reset()))\n        testing_rewards = []\n        testing_steps = []\n        for e in range(num_episodes):\n            steps, rewards, log_probs,values = self.play_episode(e)\n            self.test_rewards.append(sum(rewards))\n            self.test_steps.append(steps)\n            testing_rewards.append(sum(rewards))\n            testing_steps.append(steps)\n        print(\"Mean reward achieved : {} \".format(np.mean(testing_rewards)))\n        print(\"-\"*50)\n        if np.mean(testing_rewards) >= 200:\n            print(\"-\"*10 + \" Solved! \" + \"-\"*10)\n            print(\"Mean reward achieved : {} in {} steps\".format(np.mean(testing_rewards), np.mean(testing_steps)))\n            print(\"-\"*50)\n\n\n\nagent = A2CAgent(env)\n\n\nagent.actor.train()  #确保 network 在 training 模式\nagent.critic.train()\n\nagent.train(40000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}